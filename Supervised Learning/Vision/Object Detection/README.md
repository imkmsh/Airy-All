# Object Detection

![그림1](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Ftjymm%2FbtqArUrXnYh%2F5m9s2uQIAQ7E5ExfE1dxv0%2Fimg.jpg)

# YOLOv3

- 2015년에 발표된 You Only Look Once
- 당시에는 Faster-R-CNN이 가장 좋은 성능을 내고 있었음
- 처음으로 One-shot-detection 방법 고안
- 이전까진 Two-shot-detection, 실시간성 부족

# Faster-R-CNN

![architecture](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fdhq4iV%2FbtqBaAFDl4d%2FIZdxlDX5mkPMdnoKy2f2k0%2Fimg.png)

![simarch](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FzJoAZ%2FbtqBBU4w395%2FBMWWphbMKuo4HbzFjIM0T0%2Fimg.png)

![ARCH](https://user-images.githubusercontent.com/45263010/73349987-8b3e4980-42cf-11ea-821e-98f5437698b7.PNG)

Faster-RCNN is composed of 3 neural networks — Feature Network, Region Proposal Network (RPN), Detection Network

## Feature Network
- well known pre-trained image classification network such as VGG
- generate good features from the images
- output of this network maintains the the shape and structure of the original image

![Original image](https://miro.medium.com/max/500/1*sKw243AquatVLWsHZ5Bt6Q.png)

↓

  ![Feature](https://miro.medium.com/max/700/1*YWMVZZrj0IxVV54ig92xtg.png)
 
## RPN

RPN의 input 값은 이전 CNN 모델에서 뽑아낸 feature map  
Region proposal을 생성하기 위해 feature map위에 n x n window를 sliding window시킴  
이때, object의 크기와 비율이 어떻게 될지모르므로 k개의 anchor box를 미리 정의  
여기서는 9개의 anchor box를 이용

RPN에서 이렇게 1x1 convolution(fully connected later)을 이용하여 classification과 bbox regression을 계산  
이때 네트워크를 가볍게 만들기 위해 binary classification으로 bbox에 물체가 있나 없나만 판단  
무슨 물체인지 classification하는 것은 마지막 classification 단계에서


positive / negative examples 뽑기
![기준](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FIEkcY%2FbtqBcwbZTpn%2FvSU5RjT6EBjvUkp2mtVpfk%2Fimg.png)



- a simple network with a 3 convolutional layers
- one common layer which feeds into a two layers — one for classification and the other for bounding box regression
- generate a number of bounding boxes called Region of Interests (ROIs) that has high probability of containing any object
- output from this network is a number of bounding boxes identified by the pixel co-ordinates of two diagonal corners, and a value(1, 0, or -1, indicating whether an object is in the bounding box or not or the box can be ignored respectively)

### Training

- a number of bounding boxes are generated by a mechanism called anchor boxes
- every ‘pixel’ of the feature image is considered an anchor
- Each anchor corresponds to a larger set of squares of pixel in the original image

![그림](https://miro.medium.com/max/700/1*XEHcNRvRybLzo66F6cn-jg.png)

- anchors are positioned uniformly across both dimensions of the reshaped image
- input that is required from the feature generation layer to generate anchor boxes is the shape of the tensor, not the full feature tensor itself
- a number of rectangular boxes of different shapes and sizes are generated centered on each anchor
- usually 9 boxes are generated per anchor (3 sizes x 3 shapes)

![그림](https://miro.medium.com/max/700/1*CmOhPmqSoDCI_Yk0LWBVbg.png)

### NMS

- Non-Maximum Suppression (NMS) is used in the fist step of reduction
- NMS removes boxes that overlaps with other boxes that has higher scores ( scores are unnormalized probabilities , e.g. before softmax is applied to normalize)
- about 2000 boxes are extracted during training phase
- they are further reduced through sampling to about 256 before entering the Detection Network  

Faster R-CNN에 대한 학습이 완료된 후 RPN모델을 예측시키며 한 객체당 여러 proposal값  
NMS알고리즘을 사용하여 proposal의 개수를 줄임  

1. box들의 score(confidence)를 기준으로 정렬한다.

2. score가 가장 높은 box부터 시작해서 다른 모든 box들과 IoU를 계산해서 0.7이상이면 같은 객체를 detect한 box라고
생각할 수 있기 때문에 해당 box는 지운다. 

3. 최종적으로 각 object별로 score가 가장 높은 box 하나씩만 남게 된다.

![NMS 전](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FlAtRG%2FbtqBGgAw0Dd%2FxakhVprkQJKjnztAGjJRl1%2Fimg.png)

![NMS 후](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FbFQwuR%2FbtqBG0jD3nh%2FbhkdKFOk0PbkmWAh9qiDQ1%2Fimg.png)

- to generate labels for RPN classification, IOU of all the bounding boxes against all the ground truth boxes are taken
- the IOUs are used to label the 256 ROIs as foreground and background, and ignored
- these labels are then used to calculate the cross-entropy loss, after first removing the ignored (-1) class boxes


- Bounding box regression: the RPN also tries to tighten the center and the size of the anchor boxes around the target
- for this to happen, targets needs to be generated, and losses needs to be calculated for back propagation
- Target delta vector for the center: the distance vector from the center of the ground truth box to the anchor box is taken and normalized to the size of the anchor box
- The size target is the log of the ratio of size of each dimension of the ground truth over anchor box
- the loss is calculated by using an expression called Smooth L1 Loss


- the regular L1 loss ( e.g. the norm or absolute value) is not differentiable at 0
- smooth L1 Loss overcomes this by using L2 loss near 0
- the extent of L2 loss is tuned by a parameter called sigma

~~~
if abs(d) < 1 / sigma**2
loss = (d * sigma)**2 / 2
else
loss = abs(d) — 1 / (2 * sigma**2)
~~~

- the losses are back propagated the usual way to train RPN
- RPN can be trained by itself, or jointly with the Detection Network

## Detection Network
- input from both the Feature Network and RPN
- generates the final class and bounding box
- composed of 4 Fully Connected or Dense layers
- 2 stacked common layers shared by a classification layer and a bounding box regression layer
- to classify only the inside of the bounding boxes, the features are cropped according to the bounding boxes

### Training
- Detection Network can be considered the removed layers of the classification network that is used for features generation
- starting weights can be pre-loaded from that network before training


- IOUs of all the 2000 or so ROIs generated by the NMS following RPN against each ground truth bounding box is calculated
- the ROIs are labeled as foreground or background depending on the corresponding threshold values
- a fixed number(e.g. 256) ROIs are selected from the foreground and background ones
- If there are not enough foreground and/or background ROIs to fill the fixed number, then some ROIs are duplicated at random


- the features are cropped and scaled to 14 x 14 (max-pooled to 7 x 7 before entering the Detection Network) according to the size of the ROIs (for this, ROI width and heights are scaled to the feature size)

![그림](https://miro.medium.com/max/700/1*nYMbCEJ1OGPy6CiDYIqPPw.png)

- the set of cropped features for each image are passed through the Detection Network as a batch
- final dense layers output for each cropped feature, the score and bounding box for each class (e.g. 256 x C, 256 x 4C in one-hot encoding form, where C is the number of classes)

![bbox](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FwlAfS%2FbtqBdUdzIDo%2FekiUcXgksRellFXSBn5H6K%2Fimg.png)
![](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FHaclG%2FbtqBdUkizUl%2FOzPRkcX2FPJPFmN8BKlzl1%2Fimg.png)

- to generate label for Detection Network classification, IOUs of all the ROIs and the ground truth boxes are calculated
- depending on IOU thresholds (e.g. foreground above 0.5 , and background between 0.5 and 0.1), labels are generated for a subset of ROIs
- the difference with RPN is that here there are more classes
- classes are encoded in sparse form, instead of one-hot encoding
- following a similar approach to the RPN target generation, bounding box targets are also generated
- these targets are in the compact form as mentioned previously, hence are expanded to the one-hot encoding for calculation of loss.

![최종 loss](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FWZhx2%2FbtqBpQ9gGSl%2FeufJfGDn01qTTJWhuzgggK%2Fimg.png)

---
- i: index of an anchor  
- p i: predicted probability  
- t i : parameterized coordinates
- N cls: mini-batch size
- N reg: the number of anchor locations
- λ: balancing parameter (default = 10)
- L cls: cross-entropy loss
- L reg : L1 smooth loss
---

![bbox loss](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2Fbpk9es%2FbtqBp5kSBLg%2FikFlnDkasgvuWjSe7a6JQK%2Fimg.png)

- the loss calculation is again similar to that of the RPN network
- For classification sparse cross-entropy is used and for bounding boxes, Smooth L1 Loss is used
- the difference with RPN loss is that there are more classes (say 20 including background) to consider instead of just 2 (foreground and background)
